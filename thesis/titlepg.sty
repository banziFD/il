
\newcommand{\thesistitle}{Exploring iCaRL: Incremental Classifier and Representation Learning}
\newcommand{\thesisyear}{2018}
\newcommand{\thesisname}{Su Pengyu}
\newcommand{\thesischairadvisor}{Hong Man}
\newcommand{\committeenameA}{Yudong Yao} 
\newcommand{\thesisdepartment}{Electrical \& Computer Engineering}
\newcommand{\thesisdate}{May 3, 2018}
\newcommand{\thesistype}{Thesis}
\newcommand{\thesisdegree}{Master of Science - Computer Engineering}
\newcommand{\thesissigline}[1]{
\leftline{\hbox to 2.5in{}\hrulefill}
\endgraf
\vspace*{-18pt}
\leftline{\hbox to 2.53in{}{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Thesis / Dissertation title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\thesistitlepage}{
    \thispagestyle{empty}
    \begin{center}
    \begin{spacing}{2}
      \MakeUppercase {\thesistitle} \\
       by \\
       \thesisname \\
       A \MakeUppercase {\thesistype} \\[3mm]
     \end{spacing}
     \begin{spacing}{1}
       Submitted to the Faculty of the Stevens Institute of Technology \\ 
      in partial fulfillment of the requirements for the degree of \\[3mm]
      \end{spacing}
      \begin{spacing}{2}
    \MakeUppercase {\thesisdegree} \\
    \vspace{50mm}
     \end{spacing}
%       \strut \vfill
  \mbox{ }  \hfill \begin{minipage}{80mm}
                       \begin{spacing}{1}
                   

\noindent \rule{3.2in}{0.1mm}
\thesisname, Candidate\\[3mm]
\underline{ADVISORY COMMITTEE}\\[3mm]
\noindent \rule{3.2in}{0.1mm}\\[-1.3mm]
% for master thesis, change Chairman to Advisor
{\thesischairadvisor}, Advisor  \hfill{Date}\\[2mm]
{\noindent \rule{3.2in}{0.1mm}}\\[-1.3mm]
% for master thesis, add Reader after committeenameA
{\committeenameA}, Reader        \hfill{Date}\\[2mm]

                 	\end{spacing}
				   \end{minipage}
%     \vfill \strut				6/2009 -   changed to the following two lines
    \vspace{25mm}
	\strut
  \begin{spacing}{1}
   \MakeUppercase {Stevens Institute of Technology}\\
   Castle Point on Hudson \\
  Hoboken, NJ 07030 \\
   \thesisyear
  \end{spacing}
    \end{center}
%   \vfill \strut
    \newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Copyright Page 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\thesiscopyrightpage}{
     \thispagestyle{empty}      %no page number on the copyright page
    \strut \vfill
    \vspace*{7.2in}
   \begin{center}
        \textcopyright  \thesisyear,  \thesisname. All rights reserved.
    \end{center}
    \vfill \strut
    \newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\thesisabstract}{
\begin{center}
\MakeUppercase{\thesistitle} \\
ABSTRACT \\
\end{center}
\par Convolution Neural Network(CNN) has achieved striking success in computer vision problem such as classification, object localization, and object detection. Despite these accomplishment, CNN are still ill-equipped for incremental learning.  “Catastrophic forgetting”—an abrupt degradation of performance on the original set of classes, when new set of classes being introduced in and adapting original CNN into new training set, is one of primary barrier on the road to artificial intelligence. In this work, we explore a new training strategy that allows CNN learning about more and more concepts given by Sylvestre-Alvise Rebuffi.\cite{rebuffi} Our training strategy allows model optimization in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively.
\par Introducing a small exemplar set of original classes helps systems remember information about old classes. Although we have stored those information, a proper method feeding those knowledge needs to be carefully chosen. Geoffrey Hinton and his collaborators \cite{hinton} have shown that by distilling knowledge, we are able to transfer knowledge from previous network to new model efficiently and precisely. Combining with classic distilling knowledge and nearest mean classifier, our system shows ability of preserving old knowledge when adapting to new dataset. Knowledge mask who records known class being used in our system allows loss function can be computed in a parallel way. Experiments on CIFAR-10 and CIFAR-100 \cite{dataset}proves that our new strategy can prevent itself from accuracy degradation on old class over a long period of time.

\begin{flushleft}
Author: \thesisname \\
Advisor: \thesischairadvisor \\
Date: \thesisdate \\
Department: \thesisdepartment \\
Degree: \thesisdegree \\
\end{flushleft}
\newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%     Dedication Page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\thesisdedicationpage}{
\noindent \textbf{Dedication} \\[6mm]
    \strut \vspace{2in}
   \begin{center}
       This thesis is dedicated to all Stevens students.
    \end{center}
    \vfill \strut
    \newpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%     Acknowledgments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\thesisacknowledgments}{
\noindent \textbf{Acknowledgments} \\[6mm]

\par I would like to express my deepest appreciation to Doctor Shuanglu Dai, who provided professional guidance and teached me a great deal of computer vision research. I would especially like to thank Professor Man Hong, the advisor of me. As my mentor and teacher, he leads me into this brilliant area.
\par Nobody has been more important to me in this world than my parents, Minglong Su and Zhiru Li. To be graduate as a master's student would not have been possible without support from them. Words fail me when I want to show how lucky I was to be their son. Their love will always be my ultimate power source to pursue a spectacular life.
    \vfill \strut
    \newpage}